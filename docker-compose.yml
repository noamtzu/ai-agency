services:
  redis:
    image: redis:7-alpine
    # Keep Redis internal; do not expose to the public internet.
    expose:
      - "6379"

  gpu_server:
    build:
      context: ./gpu_server
    profiles: ["gpu"]
    environment:
      HF_TOKEN: ${HF_TOKEN:-}
      HUGGINGFACE_HUB_TOKEN: ${HUGGINGFACE_HUB_TOKEN:-}
      MODEL_ID: black-forest-labs/FLUX.2-dev
      GPU_SERVER_API_KEY: ${GPU_SERVER_API_KEY:-}
      MAX_IMAGE_BYTES: ${MAX_IMAGE_BYTES:-8388608}
      # VRAM/allocator tuning. For 24GB-class GPUs, enabling offload often avoids init OOM.
      # Values: "", "model", "sequential"
      DIFFUSERS_CPU_OFFLOAD: ${DIFFUSERS_CPU_OFFLOAD:-model}
      PYTORCH_CUDA_ALLOC_CONF: ${PYTORCH_CUDA_ALLOC_CONF:-expandable_segments:True}
      # Prevent concurrent inference requests from multiplying peak VRAM usage.
      GPU_SERVER_MAX_CONCURRENCY: ${GPU_SERVER_MAX_CONCURRENCY:-1}
      # NVIDIA container runtime hints (harmless if already set by the runtime).
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: ${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      # Persist Hugging Face / Diffusers caches to a mounted volume (models are multi-GB).
      HF_HOME: /hf_cache
      HF_HUB_CACHE: /hf_cache/hub
      TRANSFORMERS_CACHE: /hf_cache/transformers
      DIFFUSERS_CACHE: /hf_cache/diffusers
      XDG_CACHE_HOME: /hf_cache/xdg
    expose:
      - "8080"
    volumes:
      - ./backend/storage/hf_cache:/hf_cache
    # Enable GPU access (Ubuntu host requires NVIDIA drivers + NVIDIA Container Toolkit).
    # Note: `deploy:` is ignored by non-Swarm `docker compose`, so use `gpus` instead.
    gpus: all

  backend:
    build:
      context: ./backend
    environment:
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      DATABASE_URL: ${DATABASE_URL:-sqlite:///./storage/app.db}
      STORAGE_DIR: ${STORAGE_DIR:-./storage}
      # Comma-separated allowed origins (set "*" to allow all).
      # Default is "*" in backend settings (dev-friendly); set explicitly in prod.
      CORS_ORIGINS: ${CORS_ORIGINS:-}
      BACKEND_HOST: ${BACKEND_HOST:-0.0.0.0}
      BACKEND_PORT: ${BACKEND_PORT:-8000}
    volumes:
      - ./backend/storage:/app/storage
    ports:
      - "8000:8000"
    depends_on:
      - redis

  worker:
    build:
      context: ./worker
    # The default Celery concurrency scales with CPU count, which can overwhelm a single GPU server
    # (and can race model initialization). Keep it low by default; override via CELERY_CONCURRENCY.
    command: ["celery", "-A", "worker.celery_app", "worker", "--loglevel=DEBUG", "--concurrency=${CELERY_CONCURRENCY:-1}"]
    environment:
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      STORAGE_DIR: ${STORAGE_DIR:-./storage}
      # Default to the local compose GPU server (if not running/reachable, worker will fall back to mock).
      # Optional override. If unset, the worker will auto-discover a reachable GPU server from candidates.
      GPU_SERVER_URL: ${GPU_SERVER_URL:-}
      GPU_SERVER_API_KEY: ${GPU_SERVER_API_KEY:-}
      GPU_SERVER_TIMEOUT_S: ${GPU_SERVER_TIMEOUT_S:-600}
      # If GPU_SERVER_URL is explicitly set, it's used as-is. Otherwise, probe candidates.
      AUTO_GPU_SERVER: ${AUTO_GPU_SERVER:-1}
      GPU_SERVER_CANDIDATES: ${GPU_SERVER_CANDIDATES:-http://gpu_server:8080,http://127.0.0.1:8080}
    volumes:
      - ./backend/storage:/app/storage
    depends_on:
      - redis

  frontend:
    build:
      context: ./frontend
    environment:
      # If unset, the frontend will default to "same hostname on :8000" at runtime (see frontend/lib/env.ts).
      NEXT_PUBLIC_API_BASE: ${NEXT_PUBLIC_API_BASE:-}
      NEXT_PUBLIC_WS_BASE: ${NEXT_PUBLIC_WS_BASE:-}
    ports:
      - "3000:3000"
    depends_on:
      - backend
